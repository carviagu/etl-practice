---
title: "ETL Exercises Notebook"
author: "Máster Universitario en Ciencia de Datos"
date: "CUNEF Universidad"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---
<!-- Estilo y otras configuraciones -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo = FALSE}
.box {
  padding: 10px 10px;
  background-color: #E0E0E0;
    border-radius: 5px;
}

```

<!-- Fin -->

Version `r format(Sys.time(), '%d.%m.%y')`

<p align="center">
  <img width="1000" src="resources/main_panel.png">
</p>
<br/>

::: {.box}
Los enunciados de los ejercicios fueron elaborados por *Leonado Hansa* profesor de ETL en el Máster Universitario en Ciencia de Datos de CUNEF Universidad. Las soluciones propuestas han sido elaboradas por *Carlos Viñals Guitart*, al igual que el documento. 
:::

<br/>
<br/>

## General Tools

**Exercise 1**
Using ```ls``` and ```cd``` in your computer explore your folders and the contents of the folder you use for the Master's dgree. *Remark*: If you use Windows, it is recommendable downloading Git bash for Windows.

**Solution 1**

```ls``` is the bash command to show what is in a folder. For example:

```console
rstudio@c0addac18593:~/etl$ ls
code  data  exercises  logo.png  README.md
```

You can use different commands like ```-la```, this option will show you all the
elements you can find inside the directory you are:

```{console}
rstudio@c0addac18593:~/etl$ ls -la
total 56
drwxr-xr-x 6 rstudio rstudio  4096 Nov 16 12:19 .
drwxr-xr-x 1 rstudio rstudio  4096 Nov 15 15:55 ..
drwxr-xr-x 2 rstudio rstudio  4096 Oct 29 18:35 code
drwxr-xr-x 4 rstudio rstudio  4096 Oct 29 18:37 data
drwxr-xr-x 2 rstudio rstudio  4096 Nov 16 12:31 exercises
drwxr-xr-x 8 rstudio rstudio  4096 Nov 15 18:05 .git
-rw-r--r-- 1 rstudio rstudio     6 Oct 26 17:03 .gitignore
-rw-r--r-- 1 rstudio rstudio 13461 Oct 22 22:28 logo.png
-rw-r--r-- 1 rstudio rstudio   232 Oct 22 22:28 README.md
-rw-r--r-- 1 rstudio rstudio    26 Oct 26 18:31 .Rhistory
```

Without entering in detail of all the information this command gives us we can comment some 
of it. 

In this case we see the same files and folders like before, but we can also see special files like ```.git```, this files are normally hided of the users view.  And two additional files:
```.``` and ```..``` that refer to the actual directory path and the parent directory
(the one that contains the folder we are in). 

You can see also the permissions you have to the file: ```r``` read, ```w``` write
and ```x``` execute. 

```cd``` is the bash command to navigate between directories. For example:

```console
# We are now in the main folder
rstudio@c0addac18593:~/etl$ ls
code  data  exercises  logo.png  README.md

# We are going to enter the code folder
rstudio@c0addac18593:~/etl$ cd code
rstudio@c0addac18593:~/etl/code$ ls
04a-extract_csv.py  04c-extract_sql.R

# We can turn back to the parent folder
rstudio@c0addac18593:~/etl/code$ cd ..
rstudio@c0addac18593:~/etl$ ls
code  data  exercises  logo.png  README.md
```

<br/>
<br/>

**Exercise 2**
Create in your local Desktop a file called ```testing-bash.ter``` and write in the text "Holi!". Use ```echo``` and ```>``` for this. Then create a folder in your Documents folder named ```carpeta_prueba```. Move the previous file inside this folder and, check with ls you've done it properly. Now use ```cat``` for checking the text actually written in the file. Finally, remove the folder ```carpeta_prueba``` with ```rm```.

**Solution 2**

With ```echo``` the console will return the text we gave it:
```{console}
rstudio@c0addac18593:~$ echo Holi!
Holi!
```
The sign ```>``` is a pipeline that will pash the result of the command to another
function or file:
```{console}
rstudio@c0addac18593:~$ echo Holi! > testing-bash.ter
rstudio@c0addac18593:~$ ls
etl  testing-bash.ter
```
As we can see we have create a file with the text ```Holi!``` inside.
Now we create a directory with ````mkdir``` function and move the file inside with
the ```mv``` function giving the file and directory to make the transfer:
```{console}
rstudio@c0addac18593:~$ mkdir carpeta_prueba
rstudio@c0addac18593:~$ ls
carpeta_prueba  etl  testing-bash.ter
rstudio@c0addac18593:~$ mv testing-bash.ter carpeta_prueba
rstudio@c0addac18593:~$ ls
carpeta_prueba  etl
rstudio@c0addac18593:~$ cd carpeta_prueba
rstudio@c0addac18593:~/carpeta_prueba$ ls
testing-bash.ter
```
We can check whats inside of a file using ```cat``` function, this will print in 
the console the result:
```{console}
rstudio@c0addac18593:~/carpeta_prueba$ cat testing-bash.ter
Holi!
```
Finally, we are going to remove the directory.
For this will normally use ```rm``` function, but this only works for files, in the
case of directories we have to use ```rmdir```.

Another problem we can find is that the directory is not empty, in this cases we can't
simply delete it, we can use ```-rf``` property at the function, this will make a recursive
delete, deleting first the inside files and then the directory itself.

We can see our exercise now:
```{console}
rstudio@c0addac18593:~/carpeta_prueba$ cd ..
rstudio@c0addac18593:~$ rm carpeta_prueba
rm: cannot remove 'carpeta_prueba': Is a directory
rstudio@c0addac18593:~$ rmdir carpeta_prueba
rmdir: failed to remove 'carpeta_prueba': Directory not empty
rstudio@c0addac18593:~$ rm -rf carpeta_prueba
rstudio@c0addac18593:~$ ls
etl
```

<br/>
<br/>

**Exercise 3**
Run a docker container from the image ```lhansa/cunefark:0.1.0```, as shown in the
Canvas notes. Explore the container with ```ls``` and ```cd```. Create a Git repository
in GitHub and link a folder inside this container to that repo.

**Solution 3**

This exercise can be done with the tools we explain at *Exercise 1*, but it is 
interesting to talk a bit about **Docker** and **Git**. Two very important tools for 
programming in general.

***Docker***

For creating your the container of the exercise you have to open a console and write the
following command:

```console
docker run -it -p 8888:8888 lhansa/cunefark:0.1.0 bash
```

Once the image is downloaded, the container will be running, we can now if it is 
running using ```docker ps```:

```
PS C:\Users\carviagu> docker ps
CONTAINER ID   IMAGE                   COMMAND   CREATED       STATUS         PORTS                                                                                  NAMES
c0addac18593   lhansa/cunefetl:0.1.1   "bash"    3 weeks ago   Up 7 minutes   0.0.0.0:8787->8787/tcp, :::8787->8787/tcp, 0.0.0.0:8888->8888/tcp, :::8888->8888/tcp   amazing_taussig
```

We can access our container using ```docker exec``` command:

```
PS C:\Users\carviagu> docker exec -it amazing_taussig bash
rstudio@c0addac18593:/$
```

***Git***

Git is tool created to manage the evolution of our project, related with version control. This
will help to control the different stages of the evolution of our code, and make sure about the
different changes we make to it. 

You can see all related information about Git in the theory modules.

<br/>
<br/>

## Extract

### Plain Text

**Exercise 1**
With pandas, from *hipotecas_lectura* file read only the data from the second 
semester of 2020, and including somehow the names of the columns.

**Solution 1**

```{python, warning=FALSE}
import pandas as pd

# We will try with csv format
hipotecas = pd.read_csv('/home/rstudio/etl/data/hipotecas/hipotecas_lectura')

# We can see a glimpse of the object
hipotecas.head()
```

We can see that ```read_csv``` works fine but we only want the second semester of
2020:

```{python}
# WITH PYTHON
# import pandas as pd
# We will keep the column names first
hipotecas = pd.read_csv('/home/rstudio/etl/data/hipotecas/hipotecas_lectura', 
nrows=3) 
cols = list(hipotecas.columns)

# We will skip first 7 rows and read only the next 6 ones
hipotecas = pd.read_csv('/home/rstudio/etl/data/hipotecas/hipotecas_lectura', 
skiprows=7, nrows=6, names=cols)

hipotecas.head(6)
```

**Alternative solution**
```{python}
hipotecas = pd.read_csv('/home/rstudio/etl/data/hipotecas/hipotecas_lectura', skiprows= range(1, 7), nrows=6, header=0)
hipotecas
```


<br/>
<br/>

**Exercise 2**
With readr, from the ```hipotecas_lectura``` file read only the data from the first semester of 2020, and including somehow the names of the columns.

**Solution 2**

```{r}
# WITH R
library(readr)

# Searching dataframe column names
names <- names(read_csv('/home/rstudio/etl/data/hipotecas/hipotecas_lectura'))

# Reading data frame 
hipotecas2 <- read_csv('/home/rstudio/etl/data/hipotecas/hipotecas_lectura', 
                       skip = 13, n_max = 6, col_names = names)

hipotecas2
```

<br/>
<br/>

**Exercise 3**
Try your best reading the original file downloaded from the INE's site, ```hipotecas_numero_ine.csv```. *Remark*: you may need to specify that the encoding is "ISO-8859-2".

**Solution 3**
```{r}
# WITH R
hip_ine <- read.csv2('/home/rstudio/etl/data/hipotecas/hipotecas_numero_ine.csv', 
                    skip = 6, nrows = 18 , fileEncoding = 'ISO-8859-2')

head(hip_ine)
```

```{python}
# import pandas as pd
hip_ine = pd.read_csv('/home/rstudio/etl/data/hipotecas/hipotecas_numero_ine.csv', 
                      skiprows=6, nrows=18, encoding='ISO-8859-2', delimiter=';')

hip_ine.head()
```

<br/>

### Excel

**Exercise 1**
Read with Python and ```pandas``` the second sheet of the ```ejemplos_lecturas.xlsx```
file.

**Solution 1**

We can see here the solution for this exercise using **jupyter notebook**:

![](resources/exercies01.PNG)

<br/>

### SQL

**Exercise 1**
In R, download all the rows from ```IndexPrice``` in **indexKaggle.SQlite** whose
region is United States or Europe, from 2019 until the end of the period.

**Solution 1**

```{r}
library(dplyr)

# Connecting to the database
conn <- DBI::dbConnect(RSQLite::SQLite(), '../data/indexKaggle.sqlite')

# Listing tables
DBI::dbListTables(conn)

# Reading information about IndexPrice
DBI::dbListFields(conn, 'IndexPrice')
DBI::dbListFields(conn, 'IndexMeta')

# We need to join both tables so we want to know the region of the stock
# index and then extract only 2009 values
query = "
SELECT p.stock_index, date, open, high, 
low, close, adj_close, volume
FROM IndexPrice p INNER JOIN IndexMeta m ON(p.stock_index == m.stock_index)
WHERE (m.region == 'United States' OR m.region == 'Europe') AND
strftime('%Y', p.date) == '2009' 
"

table <- tbl(conn, sql(query))

table %>% collect() # will execute the query

# Closing connection
DBI::dbDisconnect(conn)
```

<br/>
<br/>

**Exercise 2**
In R and Python, from **indexKaggle.SQlite** download a table containing all the
close prices and volume since 2007 until 2010 whose currency is dollars or euros.

**Solution 2**

In **R**:
```{r}
# library(dplyr)

# Connecting to the database
conn <- DBI::dbConnect(RSQLite::SQLite(), '../data/indexKaggle.sqlite')

# We need to join both tables so we want to know the currency of the stock
# index
query <- "
SELECT p.stock_index, adj_close, volume
FROM IndexPrice p INNER JOIN IndexMeta m ON(p.stock_index == m.stock_index)
WHERE (m.currency == 'EUR' OR m.currency == 'USD') AND 
strftime('%Y', p.date) BETWEEN '2007' AND '2009'
"

table <- tbl(conn, sql(query))

table %>% collect() # will execute the query

# Closing connection
DBI::dbDisconnect(conn)
```

In **Python**:
```{python}
# import pandas as pd
from sqlalchemy import create_engine
engine = create_engine('sqlite:///../data/indexKaggle.sqlite')

query = '''
SELECT p.stock_index, adj_close, volume
FROM IndexPrice p INNER JOIN IndexMeta m ON(p.stock_index == m.stock_index)
WHERE (m.currency == 'EUR' OR m.currency == 'USD') AND 
strftime('%Y', p.date) BETWEEN '2007' AND '2009'
'''

table = pd.read_sql(query, engine)
table.head()

```

<br/>
<br/>

**Exercise 3** 
With R or Python, use the ```elections2016.sqlite``` database for extracting some data. We want a table that includes all the adjusted polls for Trump and Clinton in the Ohio and Pennsylvania states, along with the final results, order from the newest poll to the oldest (considering only the ```enddate``` column). The final table will have the next columns:

* (From the ```Polls``` table) state, enddate, grade, samplesize, adjpoll_clinton, adjpoll_trump.
* (From the ```Results``` table) electoral_votes, clinton, trump.

**Solution 3**

With **R**:
```{r}
conn <- DBI::dbConnect(RSQLite::SQLite(), '../data/elections2016.sqlite')

DBI::dbListFields(conn, 'Polls')

DBI::dbListFields(conn, 'Results')

query <- "
SELECT p.state, p.enddate, p.grade, p.samplesize, p.adjpoll_clinton, 
p.adjpoll_trump, r.electoral_votes, r.clinton, r.trump
FROM Polls p INNER JOIN Results r ON(p.state == r.state)
WHERE p.state == 'Ohio' OR p.state == 'Pennsylvania'
ORDER BY p.enddate DESC
"

table <- tbl(conn, sql(query))
table %>% collect() # will execute the query

```

With **Python**:
```{python}
# import pandas as pd
# from sqlalchemy import create_engine
engine = create_engine('sqlite:///../data/elections2016.sqlite')

query = '''
SELECT p.state, p.enddate, p.grade, p.samplesize, p.adjpoll_clinton, 
p.adjpoll_trump, r.electoral_votes, r.clinton, r.trump
FROM Polls p INNER JOIN Results r ON(p.state == r.state)
WHERE p.state == 'Ohio' OR p.state == 'Pennsylvania'
ORDER BY p.enddate DESC
'''

table = pd.read_sql(query, engine)
table.head()
```

<br/>
<br/>

**Exercise 4** 
In the **Pets** database, check if there is any owner with more the one pet.

**Solution 4**

With **R**:
```{r}
conn <- DBI::dbConnect(RSQLite::SQLite(), '../data/pets.sqlite')

DBI::dbListTables(conn)

DBI::dbListFields(conn, 'Owners')
DBI::dbListFields(conn, 'Pets')

query <- "
SELECT o.OwnerID, count(p.PetID)
FROM Owners o INNER JOIN Pets p ON(o.OwnerID == p.OwnerID)
GROUP BY o.OwnerID
HAVING count(p.PetID) > 1
"

table <- tbl(conn, sql(query))
table %>% collect() # will execute the query

```


With **Python**:
```{python}
# import pandas as pd
# from sqlalchemy import create_engine
engine = create_engine('sqlite:///../data/pets.sqlite')

query = '''
SELECT o.OwnerID, count(p.PetID)
FROM Owners o INNER JOIN Pets p ON(o.OwnerID == p.OwnerID)
GROUP BY o.OwnerID
HAVING count(p.PetID) > 1
'''

table = pd.read_sql(query, engine)
table.head()
```

<br/>
<br/>

**Exercise 5**
Calculate the income per day considering all procedures.

**Solution5**

With **R**:
```{r}
conn <- DBI::dbConnect(RSQLite::SQLite(), '../data/pets.sqlite')

DBI::dbListTables(conn)

DBI::dbListFields(conn, 'ProceduresDetails')
DBI::dbListFields(conn, 'ProceduresHistory')

query <- "
SELECT h.date, AVG(d.price)
FROM ProceduresHistory h INNER JOIN ProceduresDetails d 
    ON(h.ProcedureType == d.ProcedureType AND 
        h.ProcedureSubCode == d.ProcedureSubCode) 
GROUP BY h.date
"

table <- tbl(conn, sql(query))
table %>% collect() # will execute the query

```


With **Python**:
```{python}
# import pandas as pd
# from sqlalchemy import create_engine
engine = create_engine('sqlite:///../data/pets.sqlite')

query = '''
SELECT h.date, AVG(d.price)
FROM ProceduresHistory h INNER JOIN ProceduresDetails d 
    ON(h.ProcedureType == d.ProcedureType AND 
        h.ProcedureSubCode == d.ProcedureSubCode) 
GROUP BY h.date
'''

table = pd.read_sql(query, engine)
table.head()
```

<br/>
<br/>

**Exercise 6**
Using ```strftime()```, calculate the income per month considering only the 
transactions done by owners from the largest city in the database (the largest
city is the one with a larger number of owners)

**Solution 6**

With **R**:
```{r}
conn <- DBI::dbConnect(RSQLite::SQLite(), '../data/pets.sqlite')

query2 <- "
SELECT strftime('%Y-%m',  h.Date), AVG(d.price)
FROM ProceduresHistory h INNER JOIN ProceduresDetails d 
    ON(h.ProcedureType == d.ProcedureType AND 
        h.ProcedureSubCode == d.ProcedureSubCode) 
        INNER JOIN Pets p ON (h.PetID == p.PetID) 
        INNER JOIN Owners o ON (p.OwnerID == o.OwnerID)
WHERE o.City IN 
                (SELECT City
                  FROM Owners
                  GROUP BY City
                  ORDER BY COUNT(*) DESC
                  LIMIT 1)
GROUP BY strftime('%Y-%m',  h.Date)
"

table <- tbl(conn, sql(query2))
table %>% collect() # will execute the query
```

The same query will be used for Python but with ```sqlalchemy``` syntax.

<br/>

### Spark

Before starting this exercises, the followin preparation has been done:
```{console}
# Creating the spark session
spark = SparkSession.builder.master("local[4]") \
                    .appName('sparklyr') \
                    .getOrCreate()

flights = spark.read.csv('../data/flights/flights.csv', header=True)
flights.createOrReplaceTempView("flights")
```


**Exercise 1**
How many origin airports are there in the ```flights``` table?

**Solution 1**
```{console}
flights.select('origin').distinct().count()
```

<br/>
<br/>

**Exercise 2**
How many destinations airport are for each origin airport?

**Solution 2**
```{console}
conteo_aux = flights.select('dest', 'origin').distinct()
conteo_aux.groupBy('origin').count().show()
```

<br/>
<br/>

**Exercise 3**
Calculate de average departure delay for each origin airport.

**Solution 3**
```{console}
from pyspark.sql.types import DoubleType

flights = flights.withColumn('dep_delay', flights['dep_delay'].cast(DoubleType()))

flights.select('origin', 'dep_delay').groupBy('origin').avg('dep_delay').show()
```

<br/>
<br/>

**Exercise 4**
Remove the registers whose travelling distance is greater than the 95% of the 
travelling distances.

**Solution 4**
```{console}
from pyspark.sql.types import DoubleType

flights = flights.withColumn('distance', flights['distance'].cast(DoubleType()))

# Computing 95 quantile
flights_ord = flights.select('distance').sort('distance') # order the distances
element_95 = int(flights_ord.count() * 0.95) # position 0.95
value_95 = flights_ord.collect()[element_95][0]

# Filtering all values with distance higher than 95 quantile
flights_95 = flights.filter(flights['distance'] <= value_95)
flights_95.show(6)
```

<br/>
<br/>

## Transform

### Missing Values

**Exercise 1**
Finish replacing the **NA** values from ```df_simulated``` data frame using the 
column known distributions. For column V5 use a normal distribution with a mean 
and a variance you consider appropriate.

**Solution 1**
See ```code/05b-transform-missing.R``` document. 

```
#V1
how_many_na <- sum(is.na(df_imputed$V1))
df_imputed$V1[is.na(df_imputed$V1)] <- runif(how_many_na)

#V2
how_many_na <- sum(is.na(df_imputed$V2))
df_imputed$V2[is.na(df_imputed$V2)] <- rnorm(how_many_na, mean = 0, sd = 1)

#V3
how_many_na <- sum(is.na(df_imputed$V3))
df_imputed$V3[is.na(df_imputed$V3)] <- rnorm(how_many_na, mean = 100, sd = sqrt(10))

#v4
how_many_na <- sum(is.na(df_imputed$V4))
df_imputed$V4[is.na(df_imputed$V4)] <- rpois(how_many_na, lambda = 10)

#V5
how_many_na <- sum(is.na(df_imputed$V5))
df_imputed$V5[is.na(df_imputed$V5)] <- seq(1, 15)

```

<br/>
<br/>

**Exercise 2**
Given the next vector, replace every **NA** value with the previous non **NA** 
value.

**Solution 2**

```{r}
set.seed(5678)
vector_letters <- sample(letters, 50, TRUE)
vector_letters[sample(seq_len(50), 25)] <- NA

# Let's see the vector composition
vector_letters

# We use a as previous non NA value
prev <- "a"
pos <- 1
for (value in vector_letters) {
  if (is.na(value)) {
    vector_letters[pos] <- prev
  } else {
    prev <- value
  }
  pos <- pos + 1
}

# Check
vector_letters
```

**Alternative solution**
```{r}
library(dplyr)
set.seed(5678)
vector_letters <- sample(letters, 50, TRUE)
vector_letters[sample(seq_len(50), 25)] <- NA

lag(vector_letters, n=1)
```

<br/>
<br/>

**Exercise 3**
Replace all the **NA** of the column V5 in ```df_simulated``` data frame using the moving average approach - with a period not longer than 1. 

**Solution 3**

```
library(purrr)

df_imputed <- df_simulated

df_imputed$V5 <- imap_dbl(df_simulated$V5, function(.x, .y){
  # .x represents the value in the iteration
  # .y represents the index of the interation
  
  if(!is.na(.x)){
   return(.x) 
  } else {
  prev_value <- df_simulated$V5[.y - 1] 
  next_value <- df_simulated$V5[.y + 1] 
  
  return(mean(c(prev_value, next_value)))  
  }
})
```

<br/>
<br/>

**Exercise 4**
Build a function for scaling the ```iris``` dataset with the *min_max* approach
and scale all the numeric columns.

**Solution 4**

```{r}
library(dplyr)

# We define the min-max function approach
weight <- function(value, mini, maxi) {
  return((value-mini)/(maxi-mini))
}

# We make the conversion
iris_scaled <- iris %>% 
  mutate(across(where(is.numeric), ~ weight(., min(.), max(.))))

iris_scaled %>% 
  summarise(across(where(is.numeric), list(mean = mean, sd = sd)))
```

<br/>
<br/>

**Exercise 5**
For the data frame ```iris``` build new functions ```setosa```, ```versicolor``` 
and ```virginica```. ```setosa``` will equal 1 if ```Species == setosa``` and 0
in the other case. 

**Solution 5**

This is an exercise about categorical encoding:
```{r}
library(dplyr)

iris_encoded <- iris %>% 
  mutate(setosa = if_else(Species == 'setosa', 1, 0),
         versicolor = if_else(Species == 'versicolor', 1, 0),
         virginica = if_else(Species == 'virginica', 1, 0)) %>% 
  select(Species, setosa, versicolor, virginica)

head(iris_encoded)
```

<br/>

### Dates and Time Series

**Exercise 1**
Extract from the ```Pets``` database the daily number of procedures from the 
```ProceduresHistory``` table. 

**Solution 1**
```{r}
library(dplyr)
conn <- DBI::dbConnect(RSQLite::SQLite(), '../data/pets.sqlite')

query <- "
SELECT Date, COUNT(*)
FROM ProceduresHistory
GROUP BY Date
"

table <- tbl(conn, sql(query)) %>% collect()
head(table)
```
<br/>
<br/>

**Exercise 2**
In that table you extracted in the previous exercise, create a new column that equals
1 if the date is a Sunday; 0, elsewhere. For knowing when a date is Sunday, you can 
use something like ```format(a_date, format = "%u")```, which output the weekday number 
(7 for Sundays). **Remark**. The column must be of type ```Date```.

**Solution 2**
```{r}
library(dplyr)

table_sund <- table %>% 
  mutate(Sunday = if_else(format(as.Date(Date), format = "%u") == "7", 1, 0))

table_sund %>% filter(Sunday == 1) %>% head()
```

<br/>
<br/>

**Exercise 3**
During February 4th 2016 there was a peak, a very extreme value. Create a column
with a dummy variable indicating that date.

**Solution 4**
```{r}
library(dplyr)

table %>% 
  mutate(Anomaly = if_else(Date == '2016-02-04', 1, 0)) %>% 
  filter(format(as.Date(Date), format = '%m') == "02") %>% 
  head()
```

<br/>
<br/>

**Exercise 5**
Let’s go now with something independent from the previous data. Imagine we have a data
frame like the one created from the next code. The ﬁrst column indicates the beginning and end of each
week of 2021, but in a terrible format. Create a new column with only the ﬁrst date of each week, but with
the format ```yyyy-mm-dd```.

**Solution 5**
```{r}
library(dplyr)
crear_dias <- function(ini, fin) {
  format(seq(as.Date(ini), as.Date(fin), by = 7),
  format = "%d/%m/%Y")
}

fechas_horribles <- paste(
  crear_dias("2020-12-28", "2021-12-27"),
  crear_dias("2021-01-03", "2022-01-02"),
  sep = " - "
)

df <- tibble(
  semana = fechas_horribles,
  metrica = runif(length(fechas_horribles))
)

df

# Solution
df %>% 
  mutate(start = as.Date(semana, format = '%d/%m/%Y')) %>% 
  head()
```

<br/>

### Regular Expressions

**Exercise 1**
Translate the regex operations in R into Python. **Remark**. For replacing use 
```re.sub()``` method.

**Solution 1**
```{python}
import re
fruits = ["apple", "banana", "pear"]

r = re.compile(".*e")
newlist = list(filter(r.match, fruits))
print(newlist)

# TO DO
```

<br/>
<br/>

**Exercise 2**
Translate the regex Python operations to R.

**Solution 2**
```{r}
# TO DO
```


<br/>
<br/>

## Load

**Exercise 1**
Repeat with R all the process shown in Python for the ```indexKaggle.sqlite```
database

**Solution 1**
```{r}
library(dplyr)
library(purrr)
library(ggplot2)

conn <- DBI::dbConnect(RSQLite::SQLite(), '../data/indexKaggle.sqlite')

query = "
  SELECT IndexMeta.region, IndexPrice.stock_index, 
         IndexPrice.date, 
         IndexPrice.adj_close, IndexPrice.volume, 
         IndexMeta.currency
  FROM IndexPrice INNER JOIN IndexMeta
      ON IndexPrice.stock_index = IndexMeta.stock_index
  WHERE IndexMeta.region in ('United States', 'Europe') and 
      IndexPrice.date >= '2019-01-01' and
      IndexPrice.adj_close is not null
"

table <- tbl(conn, sql(query)) %>% collect()

head(table)

# Adj_close value to numeric
table <- table %>% 
  mutate(adj_close = as.numeric(adj_close)) %>% 
  mutate(date = as.Date(date))

table$adj_close <- imap_dbl(table$adj_close, function(.x, .y){
  # .x represents the value in the iteration
  # .y represents the index of the interation
  
  if(!is.na(.x)){
   return(.x) 
  } else {
  prev_value <- table$adj_close[.y - 1] 
  next_value <- table$adj_close[.y + 1] 
  
  return(mean(c(prev_value, next_value)))  
  }
})

# There isn't NA values
sum(is.na(table['adj_close']))

# Creating a table for each index
c(unique(table$stock_index))

  nya <- table %>% 
    filter(stock_index == 'NYA') %>% 
    select(date, adj_close) %>% 
    mutate(smoothed = zoo::rollmean(adj_close, k = 15, fill=NA))
  
  ixic <- table %>% 
    filter(stock_index == 'IXIC') %>% 
    select(date, adj_close) %>% 
    mutate(smoothed = zoo::rollmean(adj_close, k = 15, fill=NA))
  
  n100 <- table %>% 
    filter(stock_index == 'N100') %>% 
    select(date, adj_close) %>% 
    mutate(smoothed = zoo::rollmean(adj_close, k = 15, fill=NA))
  
ggplot(nya, aes(x=date)) +
  geom_line(aes(y=adj_close), color = 'blue') +
  geom_line(aes(y=smoothed), color = 'red')

# Insert tables
DBI::dbCreateTable(conn, 'NYA', nya)
DBI::dbCreateTable(conn, 'IXIC', ixic)
DBI::dbCreateTable(conn, 'N100', n100)

# Populate tables
DBI::dbAppendTable(conn, 'NYA', nya)
DBI::dbAppendTable(conn, 'IXIC', ixic)
DBI::dbAppendTable(conn, 'N100', n100)

# Check tables
DBI::dbListTables(conn)

tbl(conn, sql('SELECT * FROM NYA')) %>% collect()

# Drop tables
DBI::dbRemoveTable(conn, 'NYA')
DBI::dbRemoveTable(conn, 'IXIC')
DBI::dbRemoveTable(conn, 'N100')

# Check tables
DBI::dbListTables(conn)

```

<br/>
<br/>

## APIs

**Exercise 1**
Select 3 subgroups within the INE data base an retrieve the IPC for these subgroups,
Make the request within a loop. Somehow, manage to get a table similar to the original one. It can be done in Python or R. 

**Solution 1**
```{r}
library(httr)
library(tidyverse)

subgroups <- c('304099', '304101', '304103') # Transporte, Ocio, Hoteles

data <- list()
i = 1

for (group in subgroups) {
  # Generating the URL
  url <- paste(
    "https://servicios.ine.es/wstempus/js/ES/DATOS_METADATAOPERACION/25?p=1&g1=762:", 
    subgroups[i], 
    "&g2=3:83&g3=349:16473&date=20210531:20210901&page=1", 
    sep = "")
  
  # Petition to the API
  r <- GET(url)
  
  # Status check
  if(status_code(r) == 200) {
    # Extracting Data
    data[i] <- content(r)[[1]]$Data
    
    # Next petition
    i <- i+1
  } else {
    stop(paste('Error during petition. Code: ', status_code(r)))
  }

}

```

<br/>
<br/>

**Exercise 2**
Think of a company you are interested on extracting the following data:

* market capitalization
* PE ratio
* total revenue


**Solution 2**
```{python}
import pandas as pd
from yahoofinancials import YahooFinancials
from datetime import date

yf = YahooFinancials('TSLA')

data_mk = yf.get_market_cap()

data_pe = yf.get_pe_ratio()

data_tr = yf.get_total_revenue()

```


<br/>
<br/>

::: {.box}
<p align="center">
  Documento elaborado mediante Rmarkdown por **Carlos Viñals Guitart**
  ([carviagu](https://github.com/carviagu/etl-practice))
</p>
:::

<br/>

<p align="center">
  <img width="100" src="../logo.png">
</p>

<br/>
<br/>
